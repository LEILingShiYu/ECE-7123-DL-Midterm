{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jngXVlfmqT9A"
      },
      "source": [
        "\n",
        "\n",
        " <h1>\n",
        "Finetuning of Llama3-8b on math problem\n",
        "\n",
        "The goal is to fine-tune a Llama-3-8B model to predict if a given solution to a math problem is correct or not. Model will output True if the solution is correct, and False otherwise."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6H4hQVSqblY"
      },
      "source": [
        "## **Step 1: Install Necessary Libraries**\n",
        "\n",
        "We'll be using the unsloth library, which provides highly efficient, memory-saving training methods for large language models, making it possible to fine-tune powerful models on a single free-tier GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# %%capture\n",
        "# Since xformer and latest version of unsloth will lead to error, we just use unsloth(2025.10.10) and do not use xformer then.\n",
        "!pip install unsloth==2025.10.10"
      ],
      "metadata": {
        "id": "rpmuXH4Odl5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkuYDaVuravN"
      },
      "source": [
        "## **Step 2: Load the Model and Tokenizer**\n",
        "\n",
        "#Option1:\n",
        "We'll load the Llama-3-8B model, which is the only model permitted for this competition. We'll use Unsloth's FastLanguageModel to handle this efficiently.\n",
        "\n",
        "A key technique we'll use is 4-bit quantization (load_in_4bit = True). Think of this as compressing the model's knowledge into a much smaller file size. This significantly reduces the amount of GPU memory required, allowing us to fine-tune this large model even on a free platform like Google Colab.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "URSw7qlhqlgB"
      },
      "outputs": [],
      "source": [
        "import unsloth\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "\n",
        "max_seq_length = 1500  # Choose any sequence length\n",
        "dtype = None  # This will auto-detect the best data type for your GPU\n",
        "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
        "\n",
        "# Load the model and tokenizer from Hugging Face\n",
        "# Note: We use the base model, not a 4-bit pre-quantized one,\n",
        "# to ensure we start from the official weights.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"unsloth/Meta-Llama-3.1-8B\", # Competition-approved model\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Option2:\n",
        "Some of the finetuned models do not performed well, but we do not want to repeated the training from the start. So we load the saved checkpoints and continue the training to save time.\n",
        "\n"
      ],
      "metadata": {
        "id": "dJTENp48IHPE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e020e6b"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "max_seq_length = 1500  # Choose any sequence length\n",
        "dtype = None  # This will auto-detect the best data type for your GPU\n",
        "load_in_4bit = True  # Use 4-bit quantization to save memory\n",
        "# Define the path where the model checkpoint was saved in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint11\"\n",
        "\n",
        "# Load the model and tokenizer from the saved path\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = save_path,\n",
        "    max_seq_length = max_seq_length,\n",
        "    dtype = dtype,\n",
        "    load_in_4bit = load_in_4bit,\n",
        ")\n",
        "\n",
        "model.train()\n",
        "print(f\"Model and tokenizer loaded from: {save_path}\")"
      ],
      "metadata": {
        "id": "OCEut_gNpFUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRznzEwL3W-b"
      },
      "source": [
        "## **Step 3: Prepare the Dataset**\n",
        "\n",
        "This is a crucial step where we format our data into a structure the model can learn from. The process involves three parts:\n",
        "\n",
        "1.  **Loading**: We'll load the official competition dataset from Hugging Face.\n",
        "2.  **Splitting**: The full dataset is massive. So we just use the subset for training and validation. The range of data used can be check in the report.\n",
        "3.  **Prompting**: We will format each data sample into a clear instructional prompt. This helps the model understand its role as a mathematician verifying a solution.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "etaDwWGN3X7C"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Loading\n",
        "full_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"train\")\n",
        "shuffled_dataset = full_dataset.shuffle(seed=42)\n",
        "subset = shuffled_dataset.select(range(200000))\n",
        "\n",
        "# Splitting\n",
        "train_size = int(0.95 * len(subset))\n",
        "train_dataset = subset.select(range(train_size))\n",
        "validation_dataset = subset.select(range(train_size, len(subset)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5cL3djv3bRy"
      },
      "outputs": [],
      "source": [
        "# Prompting\n",
        "training_prompt = \"\"\"You are a mathematics expert. Carefully read the following question and its proposed solution.\n",
        "Your task is to determine whether the provided solution correctly solves the question.\n",
        "First, analyze the reasoning in the solution, then decide if the final answer is logically and mathematically correct.\n",
        "Respond with ‚ÄúTrue‚Äù or ‚ÄúFalse‚Äù and explain your reasoning briefly.\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Solution:\n",
        "{solution}\n",
        "\n",
        "Your reasoning:\n",
        "<Think step by step and explain why the solution is correct or not before giving your final answer.>\n",
        "\n",
        "Final Answer:\n",
        "{output}\"\"\"\n",
        "\n",
        "EOS_TOKEN = tokenizer.eos_token\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    questions = examples[\"question\"]\n",
        "    solutions = examples[\"solution\"]\n",
        "    outputs = examples[\"is_correct\"]\n",
        "\n",
        "    texts = [\n",
        "        training_prompt.format(\n",
        "            question=question,\n",
        "            solution=str(solution) if solution is not None else \"\",\n",
        "            output=str(output) if output is not None else \"\"\n",
        "        ) + EOS_TOKEN\n",
        "        for question, solution, output in zip(questions, solutions, outputs)\n",
        "    ]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "formatted_train_dataset = train_dataset.map(\n",
        "    formatting_prompts_func,\n",
        "    batched=True,\n",
        "    batch_size=1000,\n",
        "    num_proc=2,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8Fs1qmn37-F"
      },
      "source": [
        "## **Step 4: Configure LoRA and Set Up the Trainer**\n",
        "\n",
        "### **LoRA Configuration**\n",
        "\n",
        "Instead of training the entire model (which has billions of parameters), we'll use a technique called **Lo**w-**R**ank **A**daptation (LoRA). üéõÔ∏è\n",
        "\n",
        "Think of it like this: rather than rewriting an entire textbook, we're just adding small, efficient \"sticky notes\" (the LoRA adapters) to update the model's knowledge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lEaRjozB3tz8"
      },
      "outputs": [],
      "source": [
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 256,  # Larger rank to explain well\n",
        "    target_modules = [\n",
        "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
        "        \"gate_proj\", \"up_proj\", \"down_proj\"\n",
        "    ],\n",
        "    lora_alpha = 512,\n",
        "    lora_dropout = 0,\n",
        "\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        "    random_state = 42,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCHdotc14DgH"
      },
      "source": [
        "\n",
        "### **SFTTrainer Setup**\n",
        "\n",
        "Now we'll set up the `SFTTrainer` (Supervised Fine-tuning Trainer). This is the main tool from the `trl` library that will handle the entire training loop for us. We'll give it our model, tokenizer, dataset, and a set of training instructions, such as the batch size and number of epochs.\n",
        "\n",
        "The parameters can be checked in report."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yVZHQ4y74BCG"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=formatted_train_dataset,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=max_seq_length,\n",
        "    packing=True,\n",
        "\n",
        "    args=TrainingArguments(\n",
        "        # epoch and batch\n",
        "        per_device_train_batch_size=8,\n",
        "        gradient_accumulation_steps=2,\n",
        "        num_train_epochs = 1,\n",
        "\n",
        "        # learning rate\n",
        "        learning_rate=5e-5,\n",
        "        warmup_ratio=0.1,\n",
        "        lr_scheduler_type=\"cosine\",\n",
        "\n",
        "        # optimizer\n",
        "        optim=\"adamw_8bit\",\n",
        "        weight_decay=0.1,\n",
        "        max_grad_norm=1.0,\n",
        "        adam_beta2=0.98,\n",
        "\n",
        "        # precision\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16=torch.cuda.is_bf16_supported(),\n",
        "\n",
        "        # log and save\n",
        "        logging_steps=100,\n",
        "        output_dir = \"outputs\",\n",
        "        report_to=\"none\",\n",
        "        seed=42,\n",
        "        remove_unused_columns=True,\n",
        "        group_by_length = False,\n",
        "    ),\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTHBzKeM4zF6"
      },
      "source": [
        "## **Step 5: Start Training\\!**\n",
        "\n",
        "Now, we'll call the `train()` function on our `trainer` object. This will kick off the fine-tuning process. Based on our settings, this will run for one full epoch over our 5,000 examples.\n",
        "\n",
        "Grab a coffee, as this will take a few minutes\\! ‚òï\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JeyLJc99eXEA"
      },
      "outputs": [],
      "source": [
        "# avoid ram overflow in colab\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVrNhZ4y4zsK"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LfJfk5gIyIV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "## **Step 6: Inference and Evaluation**\n",
        "\n",
        "For large subset we chosen, the validation set is still too large when inference. So for validation, we just some samples in the subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agvQR_Ku5wWY"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "val_prompt = \"\"\"You are a mathematics expert. Carefully read the following question and its proposed solution.\n",
        "Your task is to determine whether the provided solution correctly solves the question.\n",
        "First, analyze the reasoning in the solution, then decide if the final answer is logically and mathematically correct.\n",
        "Respond with only 'True' or 'False' at the end.\n",
        "\n",
        "Question:\n",
        "{}\n",
        "\n",
        "Solution:\n",
        "{}\n",
        "\n",
        "Your reasoning:\n",
        "<Think step by step and explain why the solution is correct or not before giving your final answer.>\n",
        "\n",
        "Final Answer:\n",
        "\"\"\"\n",
        "\n",
        "def parse_prediction(response):\n",
        "    if \"Final Answer:\" in response:\n",
        "        output_part = response.split(\"Final Answer:\")[-1].strip()\n",
        "    else:\n",
        "        output_part = response\n",
        "    return 'true' in output_part.lower()\n",
        "\n",
        "val_samples = validation_dataset.select(range(1000))\n",
        "correct = 0\n",
        "batch_size = 32\n",
        "\n",
        "for i in tqdm(range(0, len(val_samples), batch_size)):\n",
        "    batch = val_samples[i:i+batch_size]\n",
        "    prompts = [val_prompt.format(q, str(s)) for q, s in zip(batch[\"question\"], batch[\"solution\"])]\n",
        "\n",
        "    inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_seq_length).to(\"cuda\")\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        outputs = model.generate(**inputs, max_new_tokens=64, do_sample=False, use_cache=True)\n",
        "\n",
        "    responses = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "    preds = [parse_prediction(r) for r in responses]\n",
        "    correct += sum(p == l for p, l in zip(preds, batch[\"is_correct\"]))\n",
        "\n",
        "accuracy = correct / len(val_samples)\n",
        "print(f\"ACC: {accuracy:.4f} ({correct}/{len(val_samples)})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ehz1Uly-JV-0"
      },
      "source": [
        "## **Step 7: Generate Submission File**\n",
        "\n",
        "This is the final step\\! We will now run our fine-tuned model on the official `test` dataset.\n",
        "\n",
        "We will loop through each example in the test set, generate a prediction, and format the results into a CSV file with two columns: `ID` and `is_correct`, as required by the competition.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "185bd13d"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from datasets import load_dataset\n",
        "FastLanguageModel.for_inference(model)\n",
        "# Load the official test set\n",
        "test_dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\", split=\"test\")\n",
        "predictions = []\n",
        "\n",
        "inference_prompt = \"\"\"You are a mathematics expert. Carefully read the following question and its proposed solution.\n",
        "Your task is to determine whether the provided solution correctly solves the question.\n",
        "First, analyze the reasoning in the solution, then decide if the final answer is logically and mathematically correct.\n",
        "Respond with only 'True' or 'False' at the end.\n",
        "\n",
        "Question:\n",
        "{}\n",
        "\n",
        "Solution:\n",
        "{}\n",
        "\n",
        "Your reasoning:\n",
        "<Think step by step and explain why the solution is correct or not before giving your final answer.>\n",
        "\n",
        "Final Answer:\n",
        "\"\"\"\n",
        "\n",
        "# A simple function to parse 'True' or 'False' from the model's raw output\n",
        "def parse_output(response_text):\n",
        "    # Find the text after \"Final Answer:\"\n",
        "    output_part = response_text.split(\"Final Answer:\")[-1]\n",
        "    # Check if \"True\" appears (case-insensitive)\n",
        "    if 'true' in output_part.lower():\n",
        "        return True\n",
        "    return False\n",
        "\n",
        "# Loop through the test dataset and generate a prediction for each example\n",
        "for example in tqdm(test_dataset):\n",
        "    question = example[\"question\"]\n",
        "    solution = example[\"solution\"]\n",
        "\n",
        "    # Format the prompt\n",
        "    prompt = inference_prompt.format(question, str(solution))\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    # Generate the prediction\n",
        "    outputs = model.generate(**inputs, max_new_tokens=64, use_cache=True)\n",
        "    response_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
        "\n",
        "    # Parse the prediction and add it to our list\n",
        "    prediction = parse_output(response_text)\n",
        "    predictions.append(prediction)\n",
        "\n",
        "# Create the submission DataFrame\n",
        "submission = pd.DataFrame({\n",
        "    'ID': range(len(predictions)),\n",
        "    'is_correct': predictions\n",
        "})\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "submission.to_csv('submission.csv', index=False)\n",
        "\n",
        "print(\"\\n‚úÖ Submission file 'submission.csv' created successfully!\")\n",
        "print(\"You can now download this file and submit it to the Kaggle competition.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe96ff59"
      },
      "source": [
        "Define the save path and save the model and tokenizer to Google Drive.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ec9d6bf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define the path to save the model checkpoint in Google Drive\n",
        "save_path = \"/content/drive/MyDrive/llama3_8b_math_verifier_checkpoint\"\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(save_path, exist_ok=True)\n",
        "\n",
        "# Save the model and tokenizer\n",
        "model.save_pretrained(save_path)\n",
        "tokenizer.save_pretrained(save_path)\n",
        "\n",
        "print(f\"Model checkpoint and tokenizer saved to: {save_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}